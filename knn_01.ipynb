{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "K-nearest neighbors (KNN) is an algorithm for classification tasks. TensorFlow is a deep learning library, and KNN is not a deep learning algorithm. For this reason, I recommend using Scikit-learn, which is a more appropriate library for this task. Below is an implementation of a KNN model using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, make_scorer, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(f\"dataset_original.csv\")\n",
    "#train = pd.read_csv(f\"dataset_160k.csv\")\n",
    "\n",
    "# shuffle the dataset\n",
    "train = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreferedOrderCat\n",
    "train.loc[train[\"PreferedOrderCat\"] == \"Laptop & Accessory\", \"PreferedOrderCat\"] = \"Laptop_Accessory\"\n",
    "train.loc[train[\"PreferedOrderCat\"] == \"Mobile Phone\", \"PreferedOrderCat\"] = \"Mobile\"\n",
    "\n",
    "#PreferredPaymentMode\n",
    "train.loc[train[\"PreferredPaymentMode\"] == \"Debit Card\", \"PreferredPaymentMode\"] = \"DebitCard\"\n",
    "train.loc[train[\"PreferredPaymentMode\"] == \"Credit Card\", \"PreferredPaymentMode\"] = \"CreditCard\"\n",
    "train.loc[train[\"PreferredPaymentMode\"] == \"CC\", \"PreferredPaymentMode\"] = \"CreditCard\"\n",
    "train.loc[train[\"PreferredPaymentMode\"] == \"E wallet\", \"PreferredPaymentMode\"] = \"Ewallet\"\n",
    "train.loc[train[\"PreferredPaymentMode\"] == \"Cash on Delivery\", \"PreferredPaymentMode\"] = \"COD\"\n",
    "\n",
    "#PreferredLoginDevice\n",
    "train.loc[train[\"PreferredLoginDevice\"] == \"Mobile Phone\", \"PreferredLoginDevice\"] = \"Mobile\"\n",
    "train.loc[train[\"PreferredLoginDevice\"] == \"Phone\", \"PreferredLoginDevice\"] = \"Mobile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'CustomerID' column since it's not useful for prediction\n",
    "X = train.drop('CustomerID', axis=1)\n",
    "\n",
    "# Separate the target variable from the rest of the dataset\n",
    "y = train['Churn']\n",
    "\n",
    "X = X.drop(columns=['Churn'], axis=1)\n",
    "#X = X.drop(columns=[], axis=1)\n",
    "\n",
    "# Perform one-hot encoding on the categorical features\n",
    "cat_cols = ['PreferredLoginDevice', 'PreferredPaymentMode', 'PreferedOrderCat','Gender','MaritalStatus']\n",
    "X = pd.get_dummies(X, columns=cat_cols)\n",
    "\n",
    "# Fill missing values with mean\n",
    "#X = X.fillna(0)\n",
    "#X.fillna(X.mode().iloc[0], inplace=True)\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "# X.fillna(X.median(), inplace=True)\n",
    "# X.fillna(method='ffill', inplace=True)\n",
    "# X.fillna(method='bfill', inplace=True)\n",
    "#X.interpolate(method='linear', inplace=True)\n",
    "\n",
    "#y = y.fillna(0)\n",
    "#y.fillna(y.mode().iloc[0], inplace=True)\n",
    "y.fillna(y.mean(), inplace=True)\n",
    "# y.fillna(y.median(), inplace=True)\n",
    "# y.fillna(method='ffill', inplace=True)\n",
    "# y.fillna(method='bfill', inplace=True)\n",
    "#y.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X.columns.tolist()\n",
    "for col in cat_cols:\n",
    "    if col in num_cols:\n",
    "        num_cols.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the numerical features using min-max scaling\n",
    "X[num_cols] = (X[num_cols] - X[num_cols].min()) / (X[num_cols].max() - X[num_cols].min())\n",
    "\n",
    "# Another way to normalize:\n",
    "#X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to binary (0 or 1)\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5630, 30)\n",
      "(5630,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(X, y, random_state=None):\n",
    "    \"\"\"\n",
    "    Applies SMOTE to the input features (X) and target variable (y) to balance the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    X: numpy array or pandas DataFrame with the input features\n",
    "    y: numpy array or pandas Series with the target variable\n",
    "    random_state: int, default=None, controls the randomness of the SMOTE algorithm\n",
    "    \n",
    "    Returns:\n",
    "    X_resampled: numpy array with the resampled input features\n",
    "    y_resampled: numpy array with the resampled target variable\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5630, 30)\n",
      "(5630,)\n"
     ]
    }
   ],
   "source": [
    "#X, y = apply_smote(X, y, random_state=42)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4504, 30) (4504,) (1126, 30) (1126,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of neighbors (k)\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the KNN model\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[930   5]\n",
      " [ 20 171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       935\n",
      "           1       0.97      0.90      0.93       191\n",
      "\n",
      "    accuracy                           0.98      1126\n",
      "   macro avg       0.98      0.94      0.96      1126\n",
      "weighted avg       0.98      0.98      0.98      1126\n",
      "\n",
      "Accuracy: 0.977797513321492\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS ACHIEVED WITHOUT CLASS BALANCING:\n",
    "\n",
    "[[908  24]\n",
    " [ 75 119]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.97      0.95       932\n",
    "           1       0.83      0.61      0.71       194\n",
    "\n",
    "    accuracy                           0.91      1126\n",
    "   macro avg       0.88      0.79      0.83      1126\n",
    "weighted avg       0.91      0.91      0.91      1126\n",
    "\n",
    "Accuracy: 0.9120781527531083\n",
    "\n",
    "\n",
    "Let's analyze the performance metrics from above:\n",
    "\n",
    "Recall (Sensitivity): Recall is the proportion of actual positive cases that were correctly identified by the model. The recall for class 1 (positive class) is 0.61, meaning that 61% of the positive cases were correctly identified. In some applications, this might be considered low, especially if false negatives have significant consequences (e.g., medical diagnosis, fraud detection). For class 0 (negative class), the recall is 0.97, which indicates that the model is better at identifying the negative class.\n",
    "\n",
    "Precision: Precision represents the proportion of predicted positive cases that were actually positive. The precision for class 1 is 0.83, meaning that 83% of the instances predicted as positive were indeed positive. For class 0, the precision is 0.92, indicating that the model is more precise in predicting the negative class.\n",
    "\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall. It's useful when you want to balance both metrics. For class 1, the F1-score is 0.71, and for class 0, it's 0.95. The F1-score shows that the model has better performance in predicting the negative class.\n",
    "\n",
    "Accuracy: The overall accuracy of the model is 0.912 (91.2%), which means that the model correctly classifies 91.2% of the cases. However, accuracy can be misleading if the dataset is imbalanced, so it's essential to look at other metrics like precision, recall, and F1-score.\n",
    "\n",
    "In general, the model performs well in classifying the negative class (class 0) but has lower performance for the positive class (class 1). Depending on the specific problem and the importance of minimizing false negatives, you may want to optimize the model for recall to improve its performance in identifying the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCAN FOR BEST PARAMETERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[917  15]\n",
      " [ 21 173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       932\n",
      "           1       0.92      0.89      0.91       194\n",
      "\n",
      "    accuracy                           0.97      1126\n",
      "   macro avg       0.95      0.94      0.94      1126\n",
      "weighted avg       0.97      0.97      0.97      1126\n",
      "\n",
      "Accuracy: 0.9680284191829485\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to search\n",
    "parameters = {'n_neighbors': list(range(1, 31))}\n",
    "\n",
    "# Create a recall scorer\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform grid search with cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(knn_classifier, parameters, scoring=recall_scorer, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "# Train the KNN model with the best hyperparameters\n",
    "knn_classifier_optimized = KNeighborsClassifier(**best_parameters)\n",
    "knn_classifier_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn_classifier_optimized.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using k = 1 we optimize our KNN model, particularly in the detection of the Positives (Churn), jumping from recall 0.61 to 0.89."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
